---
layout: post
title:  "虚拟化 Virtualization"
date:   2022-02-22 2:00:00 +0000
categories: jekyll update
---

# 虚拟化

一个正常运行着的软件系统需要软件本身、操作系统以及硬件的协同合作，因此“虚拟化”可以粗略的理解为对硬件系统的软件化，然而虚拟化不仅仅限于硬件系统，而是对特定层面资源的逻辑抽象化，使上层在使用该抽象化的资源时不会感觉和真实的资源有什么区别。一般情况下，逻辑抽象后的逻辑资源管理起来更灵活更方便，资源利用率也更高，性能会有所下降。

## 背景知识：ISA、ABI、API、UI
理解虚拟化前，需要先了解软件运行的基本架构：

![lb]({{ site.url }}/images/virtualization_1.png)

最底层最基础的是计算机的硬件层，包含诸如CPU、GPU、内存、硬盘、总线等各种看得见摸得着的设备，这些设备通过复杂的相互协作对上层提供一套标准的`指令集架构ISA`(Instruction Set Architecture)用于软件和硬件交互，指令集架构里面定义了包括指令集、数据类型、寻址、中断等基础定义。指令是计算机运行的最小功能单元，不同版本和不同型号的CPU有着不同的指令集内容，像X86和ARM的指令集就大相径庭，操作系统需要适配各种指令集的接入，这部分软件就是硬件驱动driver。ISA可理解为硬件服务接口，汇编语言就工作在这个层面。

硬件层往上就是操作系统层，操作系统提供出来的`应用二进制接口ABI`(Application Binary Interface)是软件应用所使用的低级接口。基于同类型ABI的操作系统上开发的软件可以实现相同平台的跨设备运行，从而掩盖了计算机不同硬件类型和配置的多样化。

接下来是API层，软件在开发设计过程中通常会把复杂的系统划分成小的组成部分，而其中一些最为常用的方法像文件操作的基本方法、时间操作的服务这些会被设计成运行库函数的形式，并统一对外提供编程接口，也就是的高级接口`应用程序接口API`(Application Programming Interface，API)，设计良好的API可以帮助系统各个功能组件之间更好的高内聚和低耦合，从而提高系统的弹性和韧性，管理维护起来也比较清晰。

最上面是UI层，软件正常运行后，提供给用户的人机交互界面，就是用户接口UI(User Interface)，有界面的交互接口就是GUI(Graphic User Interface)。

软件代码的编译实际上就是就是在上面这个层次架构中通过一层层的翻译解释，最终将代码这个人类语言变成指令集这个机器语言的完整过程。这样软件的运行，就实现了按照人类要求操控计算机的硬件资源完成计算并输出结果，或者保存数据等一系列作业任务。


## 计算机虚拟化

### ISA层虚拟化
ISA层虚拟化的基本工作原理是：用专们的软件来模拟不同ISA架构处理器的工作过程，将从虚拟机内发出的指令转换为宿主机认识的ISA指令，它是最原始直接的硬件仿真技术，也是性能最差的。Bochs和快速模拟器QEMU(Quick EMUlator)技术就位于这个层面，可以通过仿真技术模拟整套虚拟机的实现。
### 硬件抽象层虚拟化
硬件抽象层虚拟化是用专用软件来模拟硬件设备的工作过程，或者用软件控制将虚拟机和宿主机的特定硬件设备连通的过程。硬件抽象层虚拟化在传统的硬件和操作系统的之间引入了一层逻辑层称之为`虚拟机监视器VMM`(virtual machine monitor)，该层向上提供给虚拟机操作系统期望使用的标准接口，使得每个虚拟机操作系统往下看都觉得自己是独享了一套完整独立的硬件系统；向下则控制管理真正的硬件系统(如果包括硬件驱动也管理起来的中间层称之为`超级监督者Hypervisor`典型的如VMWare, 而通过宿主机操作系统自行管理硬件驱动，并通过调用宿主机操作系统的服务来获得资源，来实现处理器、内存和I/O设备的虚拟化称为VMM，典型的如其它非VMWare的)，VMM创建出来的虚拟机之后，通常的表现形式是宿主机操作系统的一个进程。

抽象模拟硬件设备的技术从最早的软件优先级压缩技术开始，到后来的软件二进制翻译，以及现在的硬件技术辅助虚拟化例如Intel提供的分别针对处理器、芯片组直通、网络的VT-x、VT-d和VT-c技术，开始慢慢成熟起来。

硬件抽象层虚拟化出来的虚拟机除了大大提高了对物理资源的利用率外，还增加了很多原来没有的高阶功能，例如虚拟机的热迁，在虚拟机操作系统正常运行无感知的情况下，快速从一个物理宿主机迁移到另外一个物理宿主机。业内大多IaaS平台解决方案的虚拟化关键技术都是属于这层的，例如：KVM, Xen, Vmware ESXi, Hyper-V等。

#### 插一句，说说Openstack
Openstack的底层虚拟化技术是采用了KVM+QEMU两种技术的结合，KVM是内核模块，负责CPU和内存等高性能需求硬件设备的模拟，支持硬件辅助技术的加持，而QEMU工作在用户空间，它能模拟整套计算机设备，但因为性能问题，一般用它负责除了CPU和内存外的其它设备芯片组例如网卡、显卡等的模拟，并作为对接内核态KVM的用户态程序，以实现用户对虚拟机的完整操作（用户态的应用通过系统调用进入内核态，才能操控诸如硬件设置等高级别命令），所以严格意义上讲，Openstack所采用的的虚拟机化技术是QEMU-KVM。架构上，Openstack并不直接操控KVM，而是通过libvirt这个统一的虚拟机管理API中心作为中间件来进行对KVM的管理，这样做的好处是：除了KVM外，还可以同时实现对Xen、Vmware ESXi和Hyper-V等虚拟化平台的异构能力。

### 操作系统虚拟化/ABI层虚拟化
操作系统的虚拟化并不会虚拟出另外一套操作系统，而是利用了操作系统内核提供的隔离等技术，例如LINUX操作系统下的chroot、namespace、cgroups，将特定软件运行需要的操作系统的一部分资源进行了封装（也就是对一部分ABI层进行了虚拟化），成为标准的操作系统虚拟化技术，也就是容器技术，例如：LXC、libcontainer(Docker用于替换LXC的核心模块，就是后来大名鼎鼎的runC)。操作系统虚拟化对于系统的内核资源仍然是共享的，也就是说，所有运行时中的容器在宿主机的操作系统上看只是不同的进程而已，这样虽然牺牲了一部分隔离性和兼容性，但换来的是更快的启动速度，更高的资源利用率和更轻量的软件运行负荷。容器化的操作系统封装技术已经有了业内标准化规范即Docker发起制定的OCI标准。

完全操作系统虚拟化的容器技术目前也是有的，只是这时容器和虚拟机的界限已经模糊了，例如gVisor、使用了QEMU技术的Kata-container等，它们提供了更完整也更安全的操作系统封装，但在资源利用率和性能上付出了代价。
### 库函数层虚拟化/API层虚拟化
库函数层上的虚拟化就是通过虚拟化操作系统的应用级库函数的服务接口，使得应用程序不需要修改，就可以在不同类型的操作系统中无缝运行，从而提高系统间的互操作性。库层虚拟化/APi层虚拟化使用一个运行在宿主机操作系统上的专门的软件来模拟其它操作系统实例的运行过程，该软件使用翻译的方法提供虚拟机上应用软件所需要的全部能力，这种库函数层虚拟化的落地应用有例如Windows下的运可行Linux应用程序的WSL，以及Linux下可以运行Windows的软件WINE。

### 编程语言层虚拟化/代码层虚拟化
代码层的虚拟化下，应用程序的代码首先被编译为针对其虚拟体系结构的中间代码，再由虚拟机的运行时支持系统翻译为硬件的机器语言进行执行，例如：Java虚拟机JVM。代码层虚拟化通常会受限于编程语言的类型和提供的能力。
## 网络虚拟化
网络已经有几十年的发展演变史，其分层后各司其职、逐层向上提供服务的结构和工作模式已经深入人心，业内遵循的标准有学院派OSI标准的七层协议，也有简化版的TCP/IP事实四层协议，在每个层面上，都有对应的网络设备，常见的例如：

- 一层物理层：网卡的电气参数定义，网线，集线器
- 二层链路层：网卡，网桥，交换机
- 三层网络层：路由器
- 四层传输层：主机、路由器、防火墙、网关等设备处理端口的那部分软件
- 五层会话层、六层表示层、七层应用层：软件协议和相关的数据

所以网络虚拟化任务主要集中在三层内对硬件网络设备的资源抽象化和管理，并且虚拟化网络的过程是明显的以解决某个特定问题的目的驱动型的部分虚拟化，并不完整等同于硬件网络设备的功能。

### 网卡和网线连接虚拟化
#### tun/tap

tun/tap是LINUX内核提供的一组虚拟驱动包，包含了两个虚拟化网络设备tun和tap，tun模拟三层路由器，tap模拟二层网卡，其主要的工作模式是隧道，常规用途是NAT和加密。

发往tun/tap设备的数据包从用户态进入内核态的网络协议栈后，会由tun0设备交给底层tun/tap的驱动模块tun.ko，通过字符设备/dev/net/tun（文件形式）交由用户态的应用程序进行处理，应用程序可以尽情的对拦截的数据报文进行各种修改比如增加一层加密报文头实现加密功能，典型的如IPSec的IKE/ESP加密，或者修改包地址而实现代理功能，典型的如VPN等，处理完的报文回到内核态的网络协议栈，再交给物理网卡的驱动从网卡发送出去。

#### veth

veth是LINUX为了解决通过命名空间namespace技术隔离了访问后的两个命名空间的数据报文之间有时又需要相互发送这一需求的其中一个方案，它模拟一组网线互连的网卡，两个网卡分别位于两个命名空间内，将两个veth配置好IP地址和对应的路由后，两个命名空间之间可以通信。veth在组件一个小型网络的情况下很方便，因为少经过一次协议栈性能也比tun/tap好了不少。
#### MACVLAN
在生产环境中交换机常用VLAN技术来隔离广播报文，端口可以配置为access口类型用于向下接入计算机，配置成trunk口类型用于向上汇聚传输给下一个交换机或交给路由器通过路由互联互通。Trunk口里一般有多个VLAN的数据，而对应的路由器只有一个物理口和一个MAC地址，因此路由器需要在这一个物理口上配置多个`VLAN子接口`和对应的IP，分别用来终结trunk里面的VLAN网络以及承担各个VLAN网络的网关地址的作用，这种工作模式就是单臂路由的模式，注意到一个物理网卡可以根据VLAN的不同而配置成不同的IP地址。

借鉴单臂路由的思路，MACVLAN定义一个物理网卡不仅可以通过VLAN子接口配置多个不同的IP，还可以配置多个不同的MAC地址。这个其实是网卡虚拟化的一种工作模式，虚拟后的网卡拥有不同MAC地址和IP地址的这个特征，使得MACVLAN将类似于linux bridge虚拟交换机的功能下沉到网卡成为可能。实际上MACVLAN的一个常用工作模式就是Bridge，其它工作模式包括Private, VPEA, Passthru等。

MACVLAN由linux内核模块`macvlan`提供，定义了MACVLAN接口的驱动，并支持VLAN。Bridge模式下的MACVLAN虚拟接口是可以相互高效率通信的，例如可以将同一网卡虚拟出来的MACVLAN接口放到两个命名空间内，实现容器之间的网络互通，和LINUX bridge不一样的是，MACVLAN接口无法和其父节点，也就是真实的物理网卡，之间接口互通。其它的工作模式下，表现和用途也不一样。

### 交换机虚拟化
#### linux bridge
多个命名空间之间都需要相互通信的话，使用veth就需要mesh配置，这样就很繁琐。 借鉴于真实的交换机技术，Linux发布了二层虚拟网络设备linux bridge。Linux bridge可以通过`brctl`管理命令配置接入虚拟网卡tap设备、虚拟网卡veth设备（连接linux bridge的一端不配置IP地址）和真实网卡eth，并支持交换机的所有常规功能，例如广播帧、MAC地址和端口对应表、MAC老化，STP、VLAN等。

linux bridge本身可以设置代表自己的br0接口，并配置上IP地址，在linux bridge上传输的帧如果目的MAC是br0，并且对应报文层的目的IP是br0的IP的话，这些报文就会交给linux宿主机的协议栈上层进行处理，例如拆掉包头交给应用层；对应报文层的目的IP不是br0的IP的话，根据宿主机上的路由表结果按照正确的接口转发出去。

Docker的网络默认模式就是linux brdige典型的工作模式，启动docker服务的时候会创建docker0网桥，并将后续启动的每个容器的veth对儿的一端链接到docker0网桥，同时docker0的IP地址配置为容器网络的网关，这样一个宿主机里面所有容器之间就可以实现互通，并且也通过docker0的IP路由最终经过eth0实现对外连接：iptables的SNAT配置会修改源地址IP为eth0的IP地址；协议栈会记录修改前后的信息（存于映射状态跟踪表`/proc/net/nf_conntrack`），响应报文传回来时，再将报文的目的地址修改为容器的源IP地址。
##### linux bridge的VLAN功能
Linux默认没有启动vlan功能，如果需要使用，就先加载`8021q`内核模块，然后通过`vconfig`添加vlan子接口，表现形式例如：eth0.100, 再按照需要的组网形式接入linux bridge网桥就可以了。

#### ovs/open vSwitch
ovs不是Linux的内核模块，而是专门模拟交换机的的一个软件，除了具备和linux bridge一样的功能外，它还能提供更强大的配置性、可观测性，以及更多的可扩展性，它开发实现了包括内核模块，用户模块以及可分离的管理层控制器三个层面的各种组件，将数据交换层和控制层分离，兼顾性能和管理性，支持除了VxLAN外GRE、IPSec等多种加密隧道。ovs管理起来可根据不同组件的专门功能分别使用`ovs-vsctl`，`ovs-ofctl`, `ovsdb-tool`等。

#### VXLAN协议
VLAN是现代网络最常用的协议之一，在设计之初VLAN帧头里只定义了12位VLAN ID，也就是4096个VLAN ID。 随着IDC规模的增大(例如横跨两个地区机房的同一IDC，机房之间只能通过路由连接)，虚拟化技术的发展以及云计算中心的出现，VLAN ID的数量需求远远不止几千个这样的数量级，VXLAN就是为了解决这个缺陷而出现的，它把二层的VXLAN数据帧放进了四层的UDP报文里面，然后再加上VXLAN二层帧头，将VLAN ID扩展到24位增加到16777216个，并且解决了跨中心的IDC机房在一个二层平面的需求。

VXLAN用类似隧道的方式层层增加的报文头（原来以太网帧头的14个字节，新增了包括VXLAN帧头在内的的50个头字节：外部以太网头14个字节， 外部IP头20个字节，外部UDP头8字节，VXLAN帧头8字节），隧道两头的设备被称之为VXLAN隧道终端VTE, 负责入报文时增加报文头，出报文时去掉报文头，VTE可以是虚拟设备，例如上面的linux bridge或者osv。VXLAN纯软件的这种实现同样增加了网络的复杂度，降低了传输性能。
#### 大二层虚拟网络的需求
虚拟机出现后的一个特色常用的管理功能就是虚拟机的迁移，例如：当一个物理机上的资源接近于耗尽，或者该物理机的软硬件需要做维护升级处理等，在迁移的过程中，虚拟机中的操作系统无感知（热迁移），底层的物理宿主机已经变了，但该迁移过程需要迁移前后的物理机位于同一局域网络内，这一局限性在实现中有很多问题需要考虑，例如：当一处的数据机房资源接近耗尽，而新建的数据机房在另外一个地方；或者为了保护数据安全性而需要建设的异地双活、异地容灾、两地三中心这些需求。

VXLAN协议是基于软件技术实现的大二层技术之一, 类似常用的隧道技术还有MPLS L2VPN, VPLS，GRE, NVGRE等，以及网络硬件设备厂商例如Cisco， H3C等其它专用网解决方案技术。同时，从网络运营商的角度看，这种跨数据中心的网络连接基于隧道模式的配置特点，可以实现专用设备、专用传输带宽、专用保护模式等特色的专线网络（专网、专线）提供更多一步保证，也可以提供给大客户使用。

隧道技术是数据中心网络虚拟化用的最常用的技术，在网络报文的结构上是一层对另外一层的封装，封装后进行点到点或点到多点的配置和管理。目前流行的隧道封装技术有：

- `GRE`：ANY-in-ANY
- `NVGRE`：MAC-in-IP
- `VXLAN`：MAC-in-UDP
- `STT`：MAC-in-TCP
- `VXLAN-GPE`：ANY-in-UDP
#### IPVLAN L2模式
IPVLAN和MACVLAN类似，它定义一个物理网卡（和其对应的一个MAC地址）可以虚拟出来多个共享同一MAC的接口并且每个接口可以配置不同的IP，从而节省了大量的MAC地址资源，因此也更适合和外部网络设备之间的交互。IPVLAN需要LINUX加载内核模块`ipvlan`，并且工作在二层L2模式下的IPVLAN接口需要配合工作的DHCP服务支持：按照ClientID字段而不是MAC地址字段来分配IP地址。

IPVLAN L2模式下的虚拟接口之间类似于已经配置好、连接好的linux bridge上的接口之间的互联互通：虚拟接口之间可以互通，虚拟接口对外通过父接口也就是物理口的路由转发出去。不同的是，linux bridge的这些实现和物理交换机的实现是一样的，通过网络协议栈进行处理，例如mac地址学习然后端口转发，而IPVLAN L2模式下的虚口之间的互通（单播、广播、组播）都是由IPVLAN来直接处理的，不经过宿主机网络协议栈。

### 路由器虚拟化
#### Linux"虚拟化"路由器
路由转发功能是一个Linux内核网络协议栈天生就支持的功能，可以通过命令`cat /proc/sys/net/ipv4/ip_forward`来检查是否开启了路由转发功能（就是著名的`netfilter/iptables`埋下的五个钩子函数之一`Forward`），并且通过控制命令`route -n`或者`ip r`等查看路由表信息。如果说支持路由转发功能就是路由器的话，那么不用'虚拟化'过程的Linux网络协议栈本身就是一个软件抽象出来的路由器，它可以负责该虚拟机上所有物理接口和虚拟接口的网络报文的路由传输、终结处理和管理，例如用iptables实现防火墙过滤，网络地址转换NAT, 虚拟私有网络VPN等这些路由器上常有的功能。但实际上，物理路由器的角色和功能远不止最基本的路由转发、传输和管理等功能，像位于核心网络层的路由器对动态路由协议的支持，位于接入层网络的路由器对多播路由的支持，分布式路由DVR，路由流量管理等等这些更专业的高阶功能，Linux网络协议栈还没有'虚拟化'到这些功能层面。

典型的，比如创建一个命名空间，该空间向内接入所有veth虚拟网卡对儿的其中一个，向外连接着可以联通外网的eth0，那么该命名空间就实现了虚拟路由器默认网关的功能。实际上，openstack neutron组件l3 agent服务的dnsmasq功能就是这么实现的。

#### IPVLAN L3模式
IPVLAN L3模式下，同一父接口下的所有虚拟接口之间可以在三层直接通信，即是不在一个子网内也不用配置其它信息，IPVLAN会判断并直接处理，此时IPVLAN类似于连接了所有虚拟接口的一个虚拟路由器。IPVLAN L3模式并不处理L2层的报文例如广播、组播，互通必须通过路由配置，所以适合于复杂的网络搭建，比如和大型网络使用的动态路由协议OSPF、BGP一起工作。

#### 传统网络设备硬件厂商的虚拟化网络支持
Cisco, Juniper这些传统的网络设备厂商和Vmware都有自己的虚拟网络解决方案，结合了自己的硬件网络设备或者自研的虚拟化平台，实现了更高级更完整的路由器功能，例如这些Cisco和Juniper通过在网络节点安装对应的组件支持openstack的租户网络自动编排，租户之间以及租户和外部VPN或Internet连接，并结合外部不同类型型号的网络硬件设备实现支持MPLS，VXLAN，BGP，负载均衡，防火墙等等高阶功能，整体提供了虚拟化云主机网络甚至是容器网络的一整套SDN方案。

#### 容器网络标准化CNI
Kubernets牵头制定的容器网络接口标准CNI(Container Networking Interface)，定义了容器网络的管理规范（目前只是增删网络两个操作，将来要支持动态更新，流量限速整形和网络安全策略等）和IP地址的管理规范，将网络这个重要但又专业性极强的功能的实现问题从容器中剥离出去，交给专业的网络提供商以plugin插件的形式来解决。

CNI包含了一个标准化json格式的网络配置文件，位于宿主机网络节点的目录`/etc/cni/net.d/`下，以及实现上面描述的网络和IP地址管理的二进制应用程序或者就是shell脚本，位于目录`/opt/cni/bin/`下这两部分。Kubelet/Kubenet会监听到POD创建的状态变化，并读取CNI网络配置文件获取配置信息参数，然后调用二进制应用程序对POD所在命名空间的网络进行配置连通。

支持CNI的网络插件提供商的解决方案有Open vSwitch, flannel, CALICO, weaveworks, cilium, OPENCONTRAIL(Juniper的方案)等，不通的方案有不一样的环境限制、功能特定、性能指标。借用SDN标准里面针对网络的分类（Overlay网络是上层虚拟化/逻辑抽象网络，Underlay网络是底层物理网络）, 跨主机网络的CNI实现方案可以分为：

- `Overlay模式`: 灵活易用方便配置，但性能有所下降，例如：flannel-VXLAN, Calico-IPIP, weave，cilium-VXLAN
- `路由模式`: 特殊的Overlay模式，借用了Underlay网络的路由功能，性能有所改善，例如：flannel-hostGateway, Calico-BGP, cilium-BGP
- `Underlay模式`: 性能最好，但直接依赖于底层网络设备，也需要虚拟化设备的支持，要求比较高，例如：MAC-VLAN, IP-VLAN, cilium-IPVLAN, SR-IOV

## 存储虚拟化
计算机里面的存储设备就只有硬盘，虚拟化抽象出的硬盘设备可以提高硬盘资源利用率，并使得硬盘的物理位置不在那么重要，多组物理存储资源抽象化后，就像单一的存储池供物理机本身或虚拟机使用，这就是存储里面的`池化`的概念。

一块硬盘在正常启用的过程大概分三步: `fdisk`进行分区（分割物理盘为一个或多个逻辑分区），`mkfs`格式化逻辑分区为特定的文件系统（使得操作系统可以认识），`mount`挂载使用（指定硬盘在操作系统里的根目录/入口目录）。

### LVM
逻辑卷管理LVM(logic volume managment)是linux内核提供的功能，是存储虚拟化的一个最基本的实现，它主要解决的是硬盘在不丢失数据的同时进行扩缩容的问题，提高了硬盘的利用率和管理能力。

Linux计算机作为服务器的定位，通常会挂载多块不同介质不同类型的硬盘，LVM可以先将底层真实的硬盘disk或其逻辑分区partition映射抽象为包含很多固定大小的物理扩展pe存储单元的物理卷pv；然后将这些物理卷组合池化为一个卷组vg，包含了很多pe，对应于抽象化可动态管理的一块大硬盘；将vg进行分区分成不同的逻辑卷lv；格式化lv为特定的文件系统挂载到操作系统开始读写。

LVM增加了管理维护的复杂度，比如扩缩容时需要用的reducevg, resize2fs等，同时因为增加了若干个逻辑层和需要维护对应关系，存储性能损耗很大。

### RAID
磁盘阵列，全名廉价磁盘冗余阵列 RAID(Redundant Array of Inexpensive Disks)，是另一种常用的将多组存储设备抽象为一个大逻辑硬盘的技术，这个硬盘更像真实的硬盘，你可以在上面配置LVM。RAID要解决的问题很多，因此基于硬盘不同的组合工作模式，RAID常分为如下几种模式：

- `RAID-0，串联模式`: 两个硬盘变成一个大容量的逻辑硬盘，落盘写数据两个底层硬盘各一半，读数据也是从两块硬盘各读一半。以提高硬盘读写I/O的速度为目的。
- `RAID-1，镜像模式`: 两个同容量的硬盘变成一个同容量的逻辑硬盘，落盘写数据两个底层硬盘无主从分别，都写上完整的一份，读数据从状态正常的任意一块硬盘读取。解决的是数据安全备份的问题。
- `RAID-5, 交叉分布的奇偶校验阵列`：三个同容量的硬盘变成两倍容量的逻辑硬盘并通过奇偶校验方法具备数据冗余可恢复（通过XOR计算）的安全功能，结合了RAID-0和RAID-1的两种工作模式和优点。
- `RAID-10, 串联后镜像模式`：需要四块硬盘，前两块先配置RAID-0，后两块也配置RAID-0，然后后面阵列配置为前面阵列的RAID-1镜像，同样结合了RAID-0和RAID-1的工作模式，有着数据读写速度比单盘快一倍并且可以保证数据安全性的特点。


#### 条带化 v.s. 线化
LVM和RAID-0，还有后面会提到的分布式存储系统等通过多写多读技术提高了硬盘I/O速度的方式称之为条带化，相应的其它模式例如RAID-1仍然使用了线化的I/O速度，关注的是其它问题点。
### JBOD
简单磁盘捆绑JBOD(Just Bundle Of Disks)是另一种简单的硬盘抽象化虚拟技术，它就是简单的将多个磁盘排排队，首尾标记一下并串联起来变成一块容量和的逻辑大硬盘span，数据写从第一块物理磁盘开始，写满了接着写第二块物理磁盘。性能上几乎没有损耗，但是不具备像RAID一样可以配置为数据冗余备份模式，任意一个物理硬盘损坏的话，整个span的数据都会丢失。

### 集中式存储/网络存储
#### SAN
`存储局域网SAN`（Storage Area Network）,也就是`网络磁盘阵列`，将存储的物理位置从计算机中‘剥离’了出去，集中放在远端进行管理，可以更安全的保护存储上的数据，具备专业的灾难恢复能力。SAN的网络要求具备低延迟低、高可用的特性，确保SAN的用户感觉像使用本地硬盘一样使用网络上SAN虚拟化出来的硬盘。SAN解决的是数据高速存储集中管理的要求，典型的如对性能要求较高的数据中心里的数据库应用例如Oracle，或者企业级存储这类，比较适合使用最奢华的全闪存存储SAN。

SAN对网络延迟和可靠性的苛刻要求，使得现有常见的IP网络不再适用，SAN一般使用专有网络，依据网络协议的不同，常用的有：

- `光纤通道协议FCP`(Fibre Channel Protocol)， SCSI-in-FCP
- `Internet小型计算机系统接口iSCSI`，SCI-in-IP，工作于TCP上层
- `以太网光纤通道FCoE`(Fibre Channel over Ethernet)，SCSI-in-FC-in-Ethernet，FCP和TCP/IP一根线缆共同传输的融合技术，硬伤是需要改造交换机和服务器的专用网卡，也就是通用适配器CA(Channel Adapter)，因性能表现不佳因此一直很难落地
- `基于光纤通道的非易失性内存标准FC-NVMe`(Non-Volatile Memory Express over Fibre Channel)。NVMe是为了解放闪存SSD的速度上线，建立在PCIe通道上的，比较先进的可扩展的高性能存储协议，具备低延时、IOPS高、功耗低、兼容性好等优点，是后起之秀，未来物联网IoT、人工智能AI、机器学习ML等技术的底层推动力
- `基于TCP/IP卸载引擎技术的非易失性内存标准NVMe/TCP offload`，未来会替代iSCSI
- `基于远端直接内存访问技术RoCE/IB的非易失性内存标准NVMe/RDMA RoCE/IB`，为了获取更好的传输性能，借力于NVMe的高扩展性，RDMA将bypass宿主机的网络协议栈直接与用户空间的应用进行网路数据交互

#### 背景知识：SCSI
小型计算机系统接口SCSI是计算机通过总线与其外部设备主要是存储设备之间交互的标准，有其专用的不同于TCP/IP族的从物理层一直到应用层的全部定义，按照版本的不同，传输速率可以从几十兆到几百兆不等。因为其场景的特殊性，SCSI定义了专用的电气接口和线缆，并允许串联接口从几个到十几个，传输距离最长到几米这样子等特性。

在集中式存储SAN模式下的抽象出来的远端硬盘，服务器如果想像使用本地硬盘一样，则必须遵循SCSI的协议标准。为了适配低延时和高可用的特点，服务器一般通过安装HBA FC网卡通过FCP光纤通信协议通信，或安装HBA iSCSI网卡通过高速千兆/万兆以太网通信，HBA网卡负责专门对数据进行从FCP/Ethernet/IP到SCSI的解包工作，并给服务器提供光纤口/以太网口实现对接。
#### NAS
网络附件存储NAS（Network Attached Storage）也是存储网络化的解决方案之一，是为了解决很方便的共享文件的需求而产生的。NAS的实现简单直接，集中管理的远端服务器开启服务端，计算机通过安装客户端访问服务端的文件或在本地映射目录（不是硬盘虚拟化，而是文件系统的虚拟化），传输协议使用常用的FTP、HTTP、NFS等基于TCP/IP的非常成熟的协议。NAS设备成本特别低，传输性能差，优势在于简单、方便、易用。你可以在各大在线商城上直接购物像买日用品一样挑选NAS设备，收到后简单组网就能享受你专用的网络存储服务器。

### 分布式存储/网络存储
SAN和NAS在落地应用中都属于典型的集中式存储，将存储设施独立集中管理起来，方便数据的备份、恢复和安全管控等，而分布式存储的存储设施可以位于不同的地方，甚至是不同访问速率、不同类型，它大大降低了存储的成本，有着极好的扩展性和弹性，并且性能上也不差。

分布式存储的每个存储节点都参与数据的管理和存储，某一个节点坏了的话，剩下节点组成的存储集群仍然可以正常提供服务，运维非常灵活。分布式存储的节点几乎可以无限增加，池化后对外提供一个容量巨大并且可以随时扩展、收缩的虚拟的硬盘空间，用户通过客户端获取对应的连接方法，然后就可以像访问本地的存储一样使用了。

业内常用的分布式存储例如：CEPH、GlusterFS等，它们各有各的特点，分布式存储要考虑的主要问题是因为分布式而带来的CAP平衡问题，以及相对复杂的数据备份迁移。

### 另一种分类：块存储、文件存储、对象存储

数据存储体系整个系统自下而上可以细分为包括磁盘、磁盘控制器、存储网络、磁盘阵列、卷、文件系统、目录、数据，各个层面均可进行不同程度的虚拟化。但常用的，按照存储系统对外提何种供存储类型的接口，存储主要分为块存储、文件存储 、对象存储三类，可以对应理解为块存储虚拟的是磁盘，文件存储虚拟的文件系统，对象存储虚拟的是数据。上面提到的SAN是块存储，NAS是文件存储，CEPH提供块存储ceph-rbd、文件存储ceph-FS和对象存储ceph-radosgw三种接口，而GlusterFS是基于条带化块存储技术上的文件存储接口。

块存储，典型的如硬盘，所有的数据都存储在硬盘的最小物理存储单元`扇区Sector`中，通常机械硬盘一个扇区512字节（新的机械硬盘一个扇区可以到4096字节），操作系统通过匹配的硬盘通信接口和协议驱动例如SCSI、SATA、PCIe、FC、iSCSI等对硬盘进行配置读写，为了减轻寻址的困难，操作系统抽象一个或多个连续的扇区为一个`块Block`（就像抽象内存的页一样）为最小读写单元。

文件存储，直接提供人们熟知的数据存储模式，典型的如文本、图片文件、视频文件、程序文件等，可以对这些文件进行增删改查各种操作，还有按照一定规律可寻址的目录结构这些。文件存储在逻辑层次上基于块存储但高于块存储，它将一块硬盘进行区域划分，划分出来引导区、文件分配区（FAT区，描述怎么组织的磁盘上的数据，是访问文件的指针链表集合、标识逻辑结构的目录、标识权限的文件访问标识等元）、数据存储区等。依据划分的标准不同，出现了NTFS、ext3、ext4、xfs等各种大家熟知的文件系统类型。

对象存储，以存储不方便以数据库二维逻辑展示的非结构化数据为目的，例如word，pdf，音视频等，它将每一个数据内容附加上一个用来表示其数据类型、大小、权限等内容的元数据，然后称之为一个‘对象’。对象存储没有类似文件系统的目录结构，它提供给每一个对象”身份证“信息UUID用以唯一标识，同时为了方便使用，对象存储也会提供`桶Bucket`的概念可以对对象进行逻辑分类。分布式存储系统里，对象的元数据一般集中存放，数据部分可以分散存放于多台服务器，增加I/O读写速度。业内典型的如亚马逊的S3对象存储（事实上的对象存储接口标准），直接提供Restful API提供对数据对象文件的增删查（一般没有‘改’），非常适合使用程序代码来访问数据文件的应用服务。

### 容器存储标准化CSI
和CNI一样，由kubernets联合各大厂商制定的容器存储标准CSI（Container Storage Interface）定义了如何向容器化的工作负载提供存储的机制。

早期kubernets将存储的实现放在了自己的主干代码内，内置了许多存储驱动（In-Tree模式）方便用户直接使用，因为各种弊端，最后还是将存储的实现交给了专业的存储供应商（Out-of-Tree模式）。CSI标准定义了存储设备厂商需要实现的三大部分：Identity Server, Controller Server和Node Server，其中Identity Server负责插件的注册和身份标识，Controller Server实现对存储卷的创建、attach、扩容、快照、detach、删除等，而Node Server交由各节点的kubelet调用实现mount、umount、分区格式化等，以Dameonset类型部署在各个工作节点。CSI定义的标准相当详尽，如果感兴趣可以去[CSI官方链接](https://github.com/container-storage-interface/spec/blob/master/spec.md)查询具体内容。

## SR-IOV硬件辅助I/O设备的虚拟化
Linux操作系统里，网络传输和磁盘读写都是对文件的I/O操作。常用的虚拟化解决方案里，I/O操作是由软件实现的将底层硬件抽象出来的VMM/Hypervisor这些软件管理层来拦截并通过网络/存储的虚拟化进行翻译转换处理的。为了提高传输性能，消除因为虚拟机管理层对I/O操作的干预而直接导致的延时，PCIe硬件设备厂商提出了SR-IOV技术Single Root I/O Virtualization，可以将PCIe设备的硬件PF抽象模拟为软件VF（SR-IOV的落地需要软硬件同时支持才行），从而可以直通配置挂给虚拟机使用。多个VF可共享一个PF，这样也实现了资源共享。启用了SR-IOV的虚拟机会直接和PCIe设备进行交互，降低了Host主机的CPU负担，并大大提高了I/O性能。例如：多个虚拟机可以通过挂载了实现了SR-IOV的SSD磁盘实现快速读写存储的性能，或者共享SR-IOV的硬件网卡替代linux bridge的功能并以线速传输数据。

## GPU虚拟化/vGPU
图形处理器GPU（Graphic Processing Unit）位于显卡上，原来是用来专门处理图形计算的处理器，因为屏幕上的每个像素点都需要计算处理，因此GPU天然的拥有众多的计算单元，属于典型的并行编程模型设计，适合处理大量数据类型单一的数据。而CPU相反，是串行编程模型设计，适合处理复杂的逻辑运算和不同类型的数据。GPU工作时专注于计算，不能脱离作为控制单元的CPU而单独工作。随着云计算，大数据，人工智能，物联网，虚拟货币挖矿等新兴领域的兴起，它们对海量数据进行计算的需求越来越多的交由GPU来承担起来。

原来以卓越的游戏显卡性能出身的NVIDIA，现在在新的广阔的市场需求催生下，成为了GPU以及GPU虚拟化技术的领军人物。Intel, AMD和Vmware在GPU和其虚拟化领域都有着各自的技术突破，并占有较少的一部分市场。

GPU的虚拟化目的和其它硬件资源虚拟化的目的是类似的，将物理资源GPU进行逻辑抽象化后共享给多个虚拟机或应用使用。虚拟化的方式有以下几种：

- 直通模式pass-through: 最早的GPU虚拟化模式，通过qemu模拟将一块物理GPU映射成一块虚拟GPU给某个虚拟机单独使用，性能最好，但无法共享，并且绑定宿主机。直通模式是最简单最直接的虚拟化实现。
- GPU-SRIOV模式: 需要硬件辅助支持的GPU虚拟化技术，安装了GPU设备和驱动的主机将资源抽象为PF和VF两类，PF对应硬件GPU设备并管理所有的VF设备的生命周期和调度，VF组是被逻辑分割成的多个虚拟GPU组，每一个VF可以供一个虚拟机使用，这样就实现了多个虚拟机共享使用GPU的基本功能。目前AMD非开源的提供该项功能，因为加入了逻辑控制层，性能上会有少量的损耗。
- GPU分片模式：GPU的分片既包括时分复用这种在时间上的分片，也包括对GPU资源的分片划分，例如针对一个32G显卡划分16份，那么每一个vGPU就是2G，可以挂给虚拟机使用。NVIDIA和Intel两个厂家的GPU解决方案就是GPU分片模式的虚拟化，NVIDIA的方案也是非开源的，而Intel的GVT-g方案是开源的。
